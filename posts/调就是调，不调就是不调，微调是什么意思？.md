---
date: 2025-09-30
tag: AI技术
---

# 调就是调，不调就是不调，微调是什么意思？

这两年大模型应用的落地和发展迅速，出于本地化部署成本/性能等方面考虑，很多应用选择中小模型，结合常见的两个优化方式：**微调（Fine-tuning）** 和 **RAG（检索增强生成）**，来提供AI服务。

## 大模型微调到底是什么？

打个比方：大模型就像一个普普通通的软件开发工程师，一个小P6，什么都懂点，但在某些专业问题上回答得很笼统。\
**微调**就是你再拿出一堆专业文档，帮这个工程师“恶补”某个领域的知识。

*   **全参数微调**：让他重新学习所有软件开发知识，学得最彻底，但成本极高。
*   **LoRA/QLoRA 这类参数高效微调**：直接在网上找"后端八股文"针对性学习，花费低很多，但效果也能很不错。

微调的好处：

*   学会领域专有词汇、表达习惯。

*   在特定任务上稳定发挥。

*   即使离线，也能保持较好效果。

## 微调 vs. RAG：一个会背八股，一个会查文档

看完微调的定义，你或许会有个疑惑：这事RAG不也能干？

RAG思路很直白：模型在回答前，先去知识库里翻一翻，把相关资料捞出来，再结合模型自己的理解生成答案。

来个类比：

*   **微调** = 直接教会 AI “专业知识”，以后不查资料也能说得头头是道。
*   **RAG** = AI 不一定记得所有知识，但它很会“搜和用”，保证回答有理有据。

适用场景：

*   微调：知识相对固定（行业术语、法规标准）、有一定量高质量标注数据、需要定制化风格或流程。
*   RAG：知识更新快（金融行情、技术趋势）、知识库庞大，不能全部训练进模型、需要基于最新文档来回答。
*   结合起来用：先微调模型的语言风格，再让它靠 RAG 获取最新资料。

*`小声bb一句，我感觉我就是个RAG。如果问我个八股题，比如“MySQL里用explain分析SQL性能，都会输出哪些字段，分别代表什么含义？” 或者 “常用的JVM参数都有哪些？”这种问题，我只能笼统地答个大概。真正需要用到相关知识时，都是现场查官方文档的。`*

## 微调上手实践

### 个人显卡能做多大规模的微调？（以 RTX 5090 为例）

对个人或小团队来说，硬件有限。全参数微调几乎不可能，但 LoRA/QLoRA 已经让单卡训练变得可行。

以 RTX 5090（32GB 显存） 为例：

#### 可支持的模型规模

*   7B 模型（推荐）

    *   代表：Qwen-7B、LLaMA-7B、Mistral-7B。

    *   显存需求：20–24GB 足够，32GB 很宽裕。

    *   效果：多数垂直领域问答任务已经够用。

*   13B 模型

    *   代表：Qwen-14B、LLaMA-13B。

    *   显存需求：接近 30GB，单卡能跑，但 batch size 很小，需要梯度累积。

    *   效果：回答更稳健，但训练速度慢一些。

*   数据量需求

    *   3k–10k 对问答 → 概念验证（PoC），立刻见效。

    *   2万–10万 对问答 → 能支撑一个稳定的垂直领域解答专家。

    *   10万–20万 对问答 → 覆盖率更高，但收益递减，质量比数量更关键。

举例：一条问答平均 200–300 token，5 万对问答 ≈ 1000–1500 万 token。这样的数据量，用 5090 + QLoRA 微调 7B 模型是完全可行的。

### 上手操作

作为一个b站虚拟区观众，我知道所谓"V圈"有很多虚拟主播，以及这些虚拟主播周边衍生的各种梗，属于这个圈子的"专业知识"，我将收集这些知识，微调一个"B站中文虚拟主播大模型"：qwen-vup。

先看看项目结构，一套在 单卡 RTX 5090 上跑通的端到端工程：

*   通过 MediaWiki API 获取萌娘百科的 Vtuber 相关条目
*   解析/清洗 wikitext，结构化为 `{title, info, text}`
*   生成 **Qwen** 聊天式 **SFT 问答样本**（JSONL）
*   使用 **QLoRA(4bit) + TRL** 对 **Qwen2.5-7B-Instruct** 进行指令微调
*   通过 **PEFT 适配器** 推理验证

仓库地址：<https://github.com/rainj2013/vup-llm-adapter>

#### 目录结构

    vtuber-qlora/
    ├── pyproject.toml
    ├── .gitignore
    ├── data/
    │   ├── raw_pages/          # 抓取的 .wikitext 原文
    │   └── clean_json/         # 清洗后的结构化 JSON
    ├── outputs/
    │   ├── vtuber_qa.jsonl          # 规则脚本生成的问答数据
    │   └── vtuber_qa.from_llm.jsonl # LLM 自动撰写的问答数据
    └── scripts/
        ├── fetch_pages.py          #数据抓取脚本
        ├── clean_and_struct.py     #数据清洗和结构化脚本
        ├── build_qa_jsonl.py       #基于普通文本处理构建问答对脚本
        ├── extract_with_llm.py     #基于llm构建问答对的脚本
        ├── train_qwen2.5_qlora.py  #微调训练脚本
        └── infer.py                #推理验证脚本

#### 数据获取和清洗

前面提到，微调有个要求：有一定量高质量标注数据，这其实是很难得的。恰好二次元味很浓的wiki网站 “萌娘百科” 上有大量萌萌人粉丝维护好的虚拟主播wiki页面，并且有合法的api可以直接获取数据，这里我选择从 `https://zh.moegirl.org.cn/Category:在bilibili活动过的虚拟UP主` 抓取相关数据并做进一步处理。感谢萌娘百科网站和内容的维护者们，他们和虚拟主播的切片man一样用爱发电，respect。

数据抓取下来后，是一份份类似html格式的文档：
![image.png](https://note.youdao.com/yws/res/1589/WEBRESOURCE36f1a195a4bbae2a0932b626123289b9)

需要进一步整理成结构化的json数据，这里有两个选择：

1.  直接基于文本正则匹配的方式抽取信息，由于文档都是手工编写，本身没有非常严格的格式限制，所以提取信息基本也只能提取人名+人物介绍，如果需要进一步细分信息，需要大量的人工处理。
2.  使用llm整理数据，直接让llm阅读文档，按照固定的json模板整理信息，出来的结果如下

<!---->

    {
      "name": "嘉然",
      "intro": "A-SOUL虚拟女团可爱饭缸担当，bilibili与抖音活动的虚拟UP主。",
      "debut": "2020-11-26",
      "agency": "A-SOUL（乐华娱乐）",
      "language": [
        "中文"
      ],
      "platform": [
        "bilibili",
        "抖音"
      ],
      "fandom": "嘉心糖",
      "color": "#D85B80",
      "origin": "中国",
      "birthday": null
    }

最后再基于上面的json，整理微调训练用的问答对：

    {"messages": [{"role": "system", "content": "你是懂Vtuber设定与历史的助理。回答要客观、简洁。"}, {"role": "user", "content": "【问】梨安是谁？"}, {"role": "assistant", "content": "【答】四禧丸子舞蹈担当，外表完美灵魂沙雕的虚拟UP主。。来源：https://zh.moegirl.org.cn/%E6%A2%A8%E5%AE%89"}]}
    {"messages": [{"role": "system", "content": "你是懂Vtuber设定与历史的助理。回答要客观、简洁。"}, {"role": "user", "content": "【问】梨安的出道日期是什么？"}, {"role": "assistant", "content": "【答】出道日期：2022-01-16。来源：https://zh.moegirl.org.cn/%E6%A2%A8%E5%AE%89"}]}
    {"messages": [{"role": "system", "content": "你是懂Vtuber设定与历史的助理。回答要客观、简洁。"}, {"role": "user", "content": "【问】梨安的所属社是什么？"}, {"role": "assistant", "content": "【答】所属社：四禧丸子。来源：https://zh.moegirl.org.cn/%E6%A2%A8%E5%AE%89"}]}
    {"messages": [{"role": "system", "content": "你是懂Vtuber设定与历史的助理。回答要客观、简洁。"}, {"role": "user", "content": "【问】梨安的使用语言是什么？"}, {"role": "assistant", "content": "【答】使用语言：中文。来源：https://zh.moegirl.org.cn/%E6%A2%A8%E5%AE%89"}]}
    {"messages": [{"role": "system", "content": "你是懂Vtuber设定与历史的助理。回答要客观、简洁。"}, {"role": "user", "content": "【问】梨安的活动平台是什么？"}, {"role": "assistant", "content": "【答】活动平台：Bilibili。来源：https://zh.moegirl.org.cn/%E6%A2%A8%E5%AE%89"}]}

#### 基座模型准备1

由于在hugging face上qwen官方发布的模型里面没找到Qwen3的小参数Instruct模型，这里选择了Qwen2.5-7B-Instruct 作为基座模型，首次微调训练只是先把流程走通，先不要太在意模型。
huggingface的注册、token配置不再赘述，下载模型到本地：

```
# 可选：配置一个代理国内下载更快
export HF_ENDPOINT=https://hf-mirror.com

huggingface-cli download Qwen/Qwen2.5-7B-Instruct --local-dir ./models/Qwen2.5-7B-Instruct

```

#### 用 QLoRA 训练（PEFT）

开炼，这里很多参数我也不懂，没关系，先上手再学习。x

```bash
python scripts/train_qwen2.5_qlora.py   --base_model Qwen/Qwen2.5-7B-Instruct   --data_file outputs/vtuber_qa.jsonl   --out_dir qwen25-vtuber-qlora   --max_seq_len 2048   --epochs 2   --lr 2e-4   --batch_size 1   --grad_accum 16   --lora_r 32   --lora_alpha 16   --lora_dropout 0.05
```

训练过程中显存占用16G左右，完全可以hold住，后续有需要可以调整大一点的参数：

![ae16098069437581b21287b2da1208b2.png](https://note.youdao.com/yws/res/1623/WEBRESOURCEea8dcde539afbfc26aeeb84408567306)

#### 推理验证

训练完成后，推理验证一下效果：
![image.png](https://note.youdao.com/yws/res/1629/WEBRESOURCEca9184316c2b28d06620311c3d35cfe6)

可以看到，微调有一点效果，至少知道星瞳是个vup了，但也就只知道一点。
这里肯定还有大量的优化空间，包括模型参数大小、问答对的内容、训练/推理参数等等，后续将持续学习，迭代这个微调模型。

## 名词解释

学习过程中，我遇到了一大堆不认识的名词概念，这里摘抄一下ChatGPT的解答，也供大家参考：

1.  Instruct 模型

*   定义：在基础大模型（Base Model）上，经过指令调优（Instruction Tuning）的版本。
*   特点：更擅长理解“自然语言指令”，生成符合人类期望的输出。
*   场景：对话、问答、助理类应用。
*   举例：Qwen2.5-7B-Instruct、Llama-3.1-Instruct、Mistral-Instruct。
*   区别于基础模型：基础模型更像“通用知识库”，输出常常散乱、不合规；Instruct 模型回答更规范、贴合用户提问。

1.  HF / Transformers

*   HF：Hugging Face，一个 AI 开源社区和模型库。
*   Transformers：Hugging Face 的 Python 库，是当前训练/推理 Transformer 模型的事实标准。
*   作用：
    *   直接 from\_pretrained 加载模型权重和 tokenizer；
    *   集成 LoRA、PEFT、Trainer 等微调工具；
    *   训练/推理接口与 PyTorch、DeepSpeed 等兼容。
*   场景：做微调（SFT、QLoRA、RLHF）、数据预处理、推理服务。

1.  GGUF 量化模型

*   GGUF：llama.cpp 项目的最新权重格式（取代旧的 GGML）。
*   量化：把浮点权重压缩为低精度（例如 Q4、Q5、Q8），减小显存/内存占用，提升推理速度。
*   用途：部署推理（Ollama、llama.cpp、koboldcpp 等），不能直接用于 Transformers 训练。
*   特点：
    *   文件小，适合在消费级显卡或 CPU 上本地跑。
    *   推理快，但丢失部分精度。
    *   训练/QLoRA 仍需使用 HF 原始浮点权重。

1.  LoRA / QLoRA / PEFT

*   LoRA（Low-Rank Adaptation）：在大模型的部分权重上插入可训练的低秩矩阵，减少参数更新量。
*   QLoRA（Quantized LoRA）：先把基础模型量化（比如 4bit），再在量化权重上做 LoRA 微调 → - 大幅降低显存需求。
*   PEFT（Parameter-Efficient Fine-Tuning）：一个统称，包括 LoRA、Adapter、Prefix Tuning 等方法。
*   场景：有限显存下快速适配新领域数据（如上面的 Vup QA 任务）。

1.  其他相关概念

*   SFT（Supervised Fine-Tuning）：有监督微调，用人工标注的问答/对话数据，提升模型在特定任务的表现。
*   RLHF（Reinforcement Learning with Human Feedback）：人类偏好对齐，训练过程分为奖励模型 + PPO 优化。
*   基座模型（Base Model）：未做指令调优的大模型，比如 Qwen3-8B。常用于研究或作为继续微调的起点。
*   混合专家（MoE, Mixture of Experts）：一种大模型结构，不是所有参数都同时激活，而是动态路由部分子模型，降低推理成本。

